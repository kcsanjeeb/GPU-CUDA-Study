{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CUDA Kernel for parallel reduction",
   "id": "3f9475066ed4fbaf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Summing a large array is another common problem that benefits from parallelization on a GPU. The\n",
    "basic idea behind parallel reduction is to split the array into smaller chunks and sum them in parallel.\n",
    "For example, given an array of N elements, you can have N/2 threads each sum two elements,\n",
    "reducing the problem size by half. This process repeats until there’s only one element left—the sum of\n",
    "the entire array.\n",
    "Summing Arrays: Parallel Reduction Example Consider an array of 16 random numbers. The array\n",
    "is reduced by summing pairs of elements in parallel until a single sum remains. The process is as\n",
    "follows:"
   ],
   "id": "bf152a3202ad3fef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "``` Array = [8, 3, 5, 7, 2, 9, 1, 6, 4, 10, 12, 15, 11, 14, 13, 16] ```\n",
    "\n",
    "In this example, the array is reduced from 16 elements down to 8, then to 4, then to 2, and finally to\n",
    "1, which is the sum of the entire array. At each step, pairs of elements are summed in parallel.\n",
    "The final sum of the array is 136.\n",
    "In actual GPU threads, each column corresponds to a thread, and the table shows how the values\n",
    "change at each step of the reduction. Empty cells indicate that the thread has completed its task in\n",
    "that step and is no longer active."
   ],
   "id": "59a1a5d1c9335ff6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ">## [Cuda Parallel Reuction Source Code](./cpp-cuda/parallel-reduction.cu)",
   "id": "9f183175c766744e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Code Breakdown with Detailed Explanation\n",
    "\n",
    "### Kernel Definition\n",
    "``` __global__ void reduceSum(float *input, float *output, int N) { ```\n",
    "* __global__: This function runs on the GPU but is called from CPU\n",
    "* input: Pointer to the large array we want to sum (in GPU memory)\n",
    "* output: Pointer where each block will store its partial sum\n",
    "* N: Total number of elements in the input array\n",
    "\n",
    "### 1. Shared Memory Declaration\n",
    "``` extern __shared__ float sdata[]; ```\n",
    "* What is Shared Memory?\n",
    "    * Ultra-fast memory shared by all threads in the same block\n",
    "    * ~100x faster than global GPU memory\n",
    "    * Limited size (typically 16-48KB per block)\n",
    "    * Used for thread communication within a block\n",
    "* Why use shared memory here?\n",
    "    * Each thread needs to access other threads' data during reduction\n",
    "    * Global memory accesses would be too slow\n",
    "### 2. Thread Identification\n",
    "```\n",
    "int tid = threadIdx.x;                    // Thread ID within block (0 to 255)\n",
    "int i = blockIdx.x * blockDim.x + tid;    // Global thread ID\n",
    "```\n",
    "* Example: If we have 1024 elements and 256 threads per block:\n",
    "    * Block 0: i = 0 to 255\n",
    "    * Block 1: i = 256 to 511\n",
    "    * Block 2: i = 512 to 767\n",
    "    * Block 3: i = 768 to 1023\n",
    "### 3. Loading Data into Shared Memory\n",
    "```\n",
    "sdata[tid] = (i < N) ? input[i] : 0.0f;\n",
    "__syncthreads();\n",
    "```\n",
    "* What happens:\n",
    "    * Each thread loads one element from global memory to shared memory\n",
    "        * Thread 0 loads input[0] into sdata[0]\n",
    "        * Thread 1 loads input[1] into sdata[1]\n",
    "        * Thread 255 loads input[255] into sdata[255]\n",
    "* `__syncthreads() - CRITICAL!`\n",
    "    * All threads in the block wait here until everyone finishes loading\n",
    "    * Ensures all data is available before starting reduction\n",
    "* After this step, shared memory looks like:\n",
    "```\n",
    "Thread:   0  1  2  3  4  5  6  7  8  9  10 11 12 13 14 15\n",
    "sdata:   [8, 3, 5, 7, 2, 9, 1, 6, 4, 10,12,15,11,14,13,16]\n",
    "```\n",
    "### 4. Parallel Reduction Loop (The Core Algorithm - Adjacent-pair reduction)\n",
    "```\n",
    "for (int stride = 1; stride < blockDim.x; stride *= 2) {\n",
    "   int index = 2 * stride * tid;\n",
    "   if (index < blockDim.x) {\n",
    "       if (index + stride < blockDim.x) {\n",
    "           sdata[index] += sdata[index + stride];\n",
    "       }\n",
    "   }\n",
    "   __syncthreads();\n",
    "}\n",
    "```\n",
    "\n",
    "## Code Breakdown - Main Function\n",
    "### Step 1: Define the Input Data\n",
    "```\n",
    "const int N = 16;\n",
    "float h_input[] = {8, 3, 5, 7, 2, 9, 1, 6, 4, 10, 12, 15, 11, 14, 13, 16};\n",
    "float h_output[1];  // To store the final sum\n",
    "```\n",
    "* `h_input`: Our array in CPU memory (\"h\" for host)\n",
    "* `h_output`: Will store the final result\n",
    "\n",
    "### Step 2: Allocate GPU Memory\n",
    "```\n",
    "float *d_input, *d_output;\n",
    "cudaMalloc(&d_input, N * sizeof(float));   // 16 floats on GPU\n",
    "cudaMalloc(&d_output, sizeof(float));      // 1 float on GPU\n",
    "```\n",
    "* `d_input:` GPU copy of our array (\"d\" for device)\n",
    "* `d_output:` GPU memory for the result\n",
    "\n",
    "### Step 3: Copy Data to GPU\n",
    "``` cudaMemcpy(d_input, h_input, N * sizeof(float), cudaMemcpyHostToDevice); ```\n",
    "* Copies the entire array from CPU → GPU\n",
    "* `cudaMemcpyHostToDevice:` Direction of copy\n",
    "\n",
    "### Step 4: Configure and Launch Kernel\n",
    "```\n",
    "int threads = 16;\n",
    "int sharedMem = threads * sizeof(float);  // 16 × 4 bytes = 64 bytes\n",
    "\n",
    "reduceSum<<<1, threads, sharedMem>>>(d_input, d_output, N);\n",
    "```\n",
    "* Kernel Launch Syntax: `<<<blocks, threads, sharedMem>>>`\n",
    "    * 1: 1 block (we only need one block for 16 elements)\n",
    "    * threads: 16 threads in that block\n",
    "    * sharedMem: 64 bytes of shared memory allocated\n",
    "\n",
    "### Step 5: Copy Result from GPU to CPU\n",
    "``` cudaMemcpy(h_output, d_output, sizeof(float), cudaMemcpyDeviceToHost); ```\n",
    "* `cudaMemcpy()`: CUDA function to copy memory between CPU and GPU\n",
    "* `h_output`: Destination (CPU memory where we want the result)\n",
    "* `d_output`: Source (GPU memory where the result was computed)\n",
    "* `sizeof(float)`: Copy exactly 4 bytes (size of one float)\n",
    "* `cudaMemcpyDeviceToHost`: Direction - from GPU → CPU\n",
    "```\n",
    "GPU VRAM (Device Memory)     CPU RAM (Host Memory)\n",
    "---------------------        ---------------------\n",
    "d_output[0] = 136.0    →    h_output[0] = 136.0\n",
    "```\n",
    "* The GPU computed the sum and stored it in d_output[0] (GPU memory)\n",
    "* But we can't directly use GPU memory from CPU code\n",
    "* We need to copy it back to CPU memory to print or use it"
   ],
   "id": "2554aedb074e0e2d"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
