{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Example: Sequential operations in Python\n",
    "Consider the following Python code that demonstrates how a CPU would handle a series of sequential\n",
    "operations, such as iterating through a list and performing a calculation on each item. Since\n",
    "CPUs are optimized for single-threaded operations, this is a typical example of the type of task where\n",
    "they excel."
   ],
   "id": "d05e12abc98d3f3c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T12:49:00.895821Z",
     "start_time": "2025-09-25T12:49:00.889744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "numbers = [1,2,3,4,5,6,7,8,9]\n",
    "squared_numbers = []\n",
    "\n",
    "for number in numbers:\n",
    "    squared_numbers.append(number ** 2)\n",
    "\n",
    "print(squared_numbers)"
   ],
   "id": "913ae932a9bb7309",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25, 36, 49, 64, 81]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this case, the CPU performs each iteration of the loop one after the other in a linear sequence,\n",
    "quickly handling each task."
   ],
   "id": "2f4e04eec6c2c301"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Example 2: Parallel operations in Python using TensorFlow\n",
    "In this example, we will demonstrate how to use TensorFlow to perform parallel matrix operations on a GPU. TensorFlow automatically detects available GPUs and offloads operations to them."
   ],
   "id": "c925ab8d666653e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d6be763385d6fb86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "! pip install tensorflow",
   "id": "5c9b4a0c344ef433",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T16:18:24.251698Z",
     "start_time": "2025-09-25T16:18:21.937199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "# Create a large matrix\n",
    "matrix = tf.random.uniform((1000,1000))\n",
    "print(matrix)\n",
    "# Perform a matrix multiplication (parallelized on the GPU)\n",
    "result = tf.matmul(matrix, matrix)\n",
    "tf.print(result)"
   ],
   "id": "f0ce19ef2dcf61b9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-26 00:18:22.287294: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.13004518 0.84748614 0.2270875  ... 0.7458832  0.99783456 0.5731169 ]\n",
      " [0.08732259 0.70059323 0.23242676 ... 0.7435374  0.53951645 0.8577293 ]\n",
      " [0.10347593 0.14106536 0.81996095 ... 0.8034626  0.00634098 0.1147083 ]\n",
      " ...\n",
      " [0.30464292 0.79907846 0.77350307 ... 0.95706356 0.30141973 0.77926624]\n",
      " [0.3281932  0.5348165  0.3558544  ... 0.88482213 0.19720232 0.6675515 ]\n",
      " [0.8592788  0.0202378  0.61015797 ... 0.7103808  0.74298215 0.2031815 ]], shape=(1000, 1000), dtype=float32)\n",
      "[[237.677307 242.357559 241.551941 ... 255.651245 249.1474 241.301529]\n",
      " [249.864243 245.160522 251.838593 ... 268.313385 253.124878 247.757324]\n",
      " [248.079971 247.996582 248.186905 ... 263.282562 257.262878 247.419128]\n",
      " ...\n",
      " [246.074677 249.738266 244.21106 ... 261.871704 258.541809 239.883972]\n",
      " [245.866302 251.497 249.791916 ... 267.468079 257.267151 246.39621]\n",
      " [233.316971 238.451 239.813828 ... 253.885117 243.204727 239.814]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1758817104.139776   46871 gpu_device.cc:2342] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this case, TensorFlow automatically uses the GPU to accelerate the matrix multiplication.",
   "id": "411a1b249ed1fd9c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Example 3: Parallel operations in Python using PyTorch and GPU\n",
    "This example demonstrates the use of a GPU to perform parallel operations using PyTorch, a popular deep-learning framework that provides GPU acceleration. In deep-learning, we perform matrix operations using PyTorch’s CUDA support to leverage the GPU."
   ],
   "id": "49c94d10a6acb2b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T01:49:38.932943Z",
     "start_time": "2025-09-26T01:49:38.926622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ],
   "id": "6ee00d178d300daa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## GPU in Matrix Operations\n",
    "For instance, in a machine learning context, GPUs are often used to train models that can recognize images or understand natural language. Below is an example using the PyTorch library to demonstrate how GPUs can accelerate training:"
   ],
   "id": "bf845545e9e5d1bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T01:49:42.726415Z",
     "start_time": "2025-09-26T01:49:41.359265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Sample Tensor\n",
    "data = torch.randn(1000,1000).to(device)\n",
    "\n",
    "# Perform a tensor operation\n",
    "result = data * data\n",
    "print(result)"
   ],
   "id": "e5c9ac6a759f4d33",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.3159e+00, 2.3972e+00, 7.3035e-01,  ..., 2.3215e+00, 1.5452e+00,\n",
      "         3.2903e+00],\n",
      "        [5.8138e-01, 1.8858e-01, 1.0537e-02,  ..., 2.0386e-02, 1.2114e+00,\n",
      "         5.2845e-01],\n",
      "        [1.1759e+00, 9.0369e-01, 2.4117e-03,  ..., 2.0131e-01, 5.2694e+00,\n",
      "         2.9426e-01],\n",
      "        ...,\n",
      "        [2.2667e-01, 3.4197e+00, 2.1350e-02,  ..., 1.5322e+00, 1.4658e+00,\n",
      "         6.8606e-02],\n",
      "        [1.2529e-01, 1.9850e+00, 2.1910e+00,  ..., 2.0396e+00, 3.6126e-03,\n",
      "         3.4626e-02],\n",
      "        [2.7877e-02, 9.7797e-01, 2.4776e+00,  ..., 3.7252e-04, 1.6705e+00,\n",
      "         1.4745e+00]], device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this example, if a GPU is available, the tensor operations will be performed on it, speeding up the\n",
    "computation."
   ],
   "id": "db57d4d6b16b544"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Architecture Comparision\n",
    "The CPU and GPU architectures differ fundamentally in their design and purpose. While CPUs have\n",
    "fewer cores, each core is highly sophisticated and capable of handling complex instructions. This\n",
    "makes CPUs ideal for managing general-purpose tasks, with the control unit acting as a “leader”, coordinating\n",
    "the system. On the other hand, GPUs are equipped with a large number of simple, lightweight\n",
    "cores that excel at parallel processing. The GPU architecture is designed for handling large amounts of\n",
    "simple, repetitive tasks, functioning more like “workers” in a large team, efficiently executing multiple\n",
    "tasks simultaneously.\n",
    "\n",
    "In the CPU diagram, the control unit coordinates the smaller number of Arithmetic Logic Units\n",
    "(ALUs) to perform general-purpose computation. The IO and cache systems support data transfer\n",
    "and storage, enabling the CPU to handle a wide range of complex tasks.\n",
    "\n",
    "In the GPU diagram, the architecture emphasizes a much larger number of simple cores. Each core\n",
    "is optimized for performing specific, simple tasks in parallel, which is ideal for graphics rendering and\n",
    "other highly parallel computations. This design trades off individual core power for sheer numbers,\n",
    "focusing on throughput over latency."
   ],
   "id": "5454ca3cbce1b8ad"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
