{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Vector Addition: The Fundamentals",
   "id": "3b99fb84e64ad23a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "One of the simplest operations you can perform on a GPU is vector addition. This involves adding two\n",
    "arrays element-wise. The advantage of using a GPU is that you can perform multiple operations in\n",
    "parallel, which can drastically improve performance.\n",
    "In CUDA, we use threads to perform the addition of the vectors. Each thread handles the computation\n",
    "for one element in the vector. The idea is to map each thread to one data element, allowing the\n",
    "GPU to compute the sum of the entire array in parallel.\n",
    "\n",
    "Let’s assume we have two vectorsAandB, each withN elements. The task is to add them together\n",
    "to produce a third vector C, where C[i] = A[i] + B[i]."
   ],
   "id": "af56f97799084690"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[./cpp-cuda/vector-addition.cu](./cpp-cuda/vector-addition.cu)",
   "id": "c58908f865972201"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 1: Understanding the CUDA Kernel\n",
    "\n",
    "### Kernel Definition:\n",
    "```__global__ void vectorAdd(float *A, float *B, float *C, int N) {```\n",
    "* `__global__:` This is a GPU function that can be called from CPU\n",
    "* `void vectorAdd:` Function name - runs on GPU but called from CPU\n",
    "* `Parameters:` Pointers to arrays A, B, C and size N\n",
    "\n",
    "### Thread Index Calculation:\n",
    "```int i = threadIdx.x + blockIdx.x * blockDim.x;```\n",
    "* This is the most important line in CUDA! Let's understand each part:\n",
    "``` Grid (multiple blocks)\n",
    "┌───────────┬───────────┬───┐\n",
    "│ Block 0   │ Block 1   │...│  ← blockIdx.x (0, 1, 2...)\n",
    "│           │           │   │\n",
    "│ Threads:  │ Threads:  │   │\n",
    "│ 0,1,2...  │ 0,1,2...  │   │  ← threadIdx.x (0,1,2...)\n",
    "└───────────┴───────────┴───┘\n",
    "```\n",
    "* `threadIdx.x`: Thread index within its block (0 to 255 if 256 threads/block)\n",
    "* `blockIdx.x`: Which block this thread is in (0, 1, 2...)\n",
    "* `blockDim.x`: How many threads per block (256 in our example)\n",
    "* `i`: Global index across all threads\n",
    "* **Example**: If you have 1024 elements and 256 threads/block:\n",
    "    * Block 0: threads 0-255 → i = 0-255\n",
    "    * Block 1: threads 0-255 → i = 256-511\n",
    "    * Block 2: threads 0-255 → i = 512-767\n",
    "    * Block 3: threads 0-255 → i = 768-1023\n",
    "\n",
    "### Boundary Check:\n",
    "```\n",
    "if (i < N) {\n",
    "    C[i] = A[i] + B[i];\n",
    "}\n",
    "```\n",
    "* Why needed? We might launch more threads than array elements\n",
    "* Example: For N=1024 with 256 threads/block, we need 4 blocks (1024 threads) - perfect!\n",
    "* But if N=1000, we still use 4 blocks (1024 threads), but last 24 threads skip work"
   ],
   "id": "4e4f517aed08902"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 2: Host (CPU) Code - Step by Step\n",
    "### Step 1: Define Problem Size\n",
    "```\n",
    "int N = 1024;                    // 1024 elements\n",
    "size_t size = N * sizeof(float); // Total memory needed: 1024 × 4 bytes = 4096 bytes\n",
    "```\n",
    "* `size_t` is a special data type that represents sizes and counts in C/C++. It's an unsigned integer type that's guaranteed to be large enough to represent the size of any object in memory.\n",
    "* `sizeof(float)` returns how many bytes a float occupies\n",
    "* `N * sizeof(float)`\n",
    "    * `N` is the number of elements you want\n",
    "    * `sizeof(float)` is bytes per element\n",
    "    * Multiplication gives total bytes needed for the array `6 × 4 = 24 bytes`\n",
    "### Step 2: Allocate CPU Memory\n",
    "```\n",
    "CPU RAM (Host)         GPU VRAM (Device)\n",
    "┌─────────────┐        ┌─────────────┐\n",
    "│ h_A: 1,2,3..│        │ d_A: ? ? ? ?│ ← Allocated but empty\n",
    "│ h_B:10,20,30│        │ d_B: ? ? ? ?│\n",
    "│ h_C: ? ? ? ?│        │ d_C: ? ? ? ?│\n",
    "└─────────────┘        └─────────────┘\n",
    "```\n",
    "```\n",
    "float *h_A = (float *)malloc(size);  // Allocate array A on CPU\n",
    "float *h_B = (float *)malloc(size);  // Allocate array B on CPU\n",
    "float *h_C = (float *)malloc(size);  // Allocate result array C on CPU\n",
    "```\n",
    "* The h_ prefix typically indicates that these arrays are stored in host memory (CPU RAM), as opposed to d_ which would indicate device memory (GPU memory). This is a common convention in CUDA programming.\n",
    "    * h_A: Array with 6 elements initialized to {1, 2, 3, 4, 5, 6} `h_A: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]`\n",
    "    * h_B: Array with 6 elements initialized to {10, 20, 30, 40, 50, 60} `h_B: [10.0, 20.0, 30.0, 40.0, 50.0, 60.0]`\n",
    "    * h_C: Array with 6 elements (uninitialized, will contain garbage values) `h_C: [?, ?, ?, ?, ?, ?]  // Uninitialized values`\n",
    "* `malloc():` Standard C function to allocate CPU memory\n",
    "* `h_ prefix:` Convention for \"host\" (CPU) pointers\n",
    "* This pattern is often seen in GPU programming where:\n",
    "    * h_A and h_B contain input data on the CPU\n",
    "    * h_C is allocated to receive results after some computation (likely on GPU)\n",
    "    * The arrays would later be copied to device memory for processing\n",
    "\n",
    "### Step 3: Initialize Data on CPU\n",
    "```\n",
    "for (int i = 0; i < N; i++) {\n",
    "    h_A[i] = i;      // A = [0, 1, 2, 3, ..., 1023]\n",
    "    h_B[i] = i * 2;  // B = [0, 2, 4, 6, ..., 2046]\n",
    "}\n",
    "```\n",
    "### Step 4: Allocate GPU Memory\n",
    "```\n",
    "float *d_A, *d_B, *d_C;           // Declare GPU pointers\n",
    "cudaMalloc(&d_A, size);           // Allocate array A on GPU\n",
    "cudaMalloc(&d_B, size);           // Allocate array B on GPU\n",
    "cudaMalloc(&d_C, size);           // Allocate result array C on GPU\n",
    "```\n",
    "* `cudaMalloc():` CUDA function to allocate GPU memory\n",
    "* `d_ prefix:` Convention for \"device\" (GPU) pointers\n",
    "* GPU memory is separate from CPU memory!\n",
    "### Step 5: Copy Data from CPU → GPU\n",
    "```\n",
    "cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);  // CPU → GPU\n",
    "cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);  // CPU → GPU\n",
    "```\n",
    "* `cudaMemcpy():` Copies data between CPU and GPU\n",
    "* `cudaMemcpyHostToDevice:` Direction - from CPU to GPU\n",
    "* This is slow (PCIe bottleneck) - minimize these transfers!\n",
    "### Step 6: Configure and Launch Kernel\n",
    "```\n",
    "int threadsPerBlock = 256;\n",
    "int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;\n",
    "vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);\n",
    "```\n",
    "* Kernel launch syntax: `<<<blocks, threads>>>`\n",
    "* Calculation:\n",
    "    * ``` blocksPerGrid = (1024 + 256 - 1) / 256 = (1279) / 256 = 4.99 → 4 (integer division) ```\n",
    "    * 4 blocks × 256 threads/block = 1024 threads total\n",
    "    * Each thread processes one array element\n",
    "### Step 7: Copy Results from GPU → CPU\n",
    "``` cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);  // GPU → CPU ```\n",
    "* `cudaMemcpyDeviceToHost:` Direction - from GPU to CPU\n",
    "* Now `h_C` contains the results computed by GPU\n",
    "### Step 8: Cleanup Memory\n",
    "```\n",
    "cudaFree(d_A);  // Free GPU memory\n",
    "cudaFree(d_B);\n",
    "cudaFree(d_C);\n",
    "// Also should free CPU memory: free(h_A); free(h_B); free(h_C);\n",
    "```"
   ],
   "id": "578620235d7b21d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 3: Visual Execution Timeline\n",
    "\n",
    "### CPU Execution (Sequential):\n",
    "```\n",
    "Time:    Operation\n",
    "↓\n",
    "t1:      Allocate CPU memory (h_A, h_B, h_C)\n",
    "t2:      Initialize arrays on CPU\n",
    "t3:      Allocate GPU memory (d_A, d_B, d_C)\n",
    "t4:      Copy h_A → d_A, h_B → d_B (CPU→GPU)\n",
    "t5:      LAUNCH KERNEL (GPU takes over)\n",
    "t6:      Wait for GPU to finish\n",
    "t7:      Copy d_C → h_C (GPU→CPU)\n",
    "t8:      Free memory\n",
    "```\n",
    "\n",
    "### GPU Execution (Parallel):\n",
    "```\n",
    "Time:    All 1024 threads execute SIMULTANEOUSLY!\n",
    "↓\n",
    "t5:      Thread 0:   C[0] = A[0] + B[0]\n",
    "         Thread 1:   C[1] = A[1] + B[1]\n",
    "         Thread 2:   C[2] = A[2] + B[2]\n",
    "         ...\n",
    "         Thread 1023: C[1023] = A[1023] + B[1023]\n",
    "         ALL HAPPEN AT THE SAME TIME!\n",
    "```\n"
   ],
   "id": "d7dbf068cfc0160"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
