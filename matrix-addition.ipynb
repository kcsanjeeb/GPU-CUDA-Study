{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Matrix Addition\n",
    "Matrix addition is an element-wise operation, which makes it highly parallelizable. Each element in\n",
    "the resulting matrix can be computed independently. Using Python, we can parallelize this operation\n",
    "using the concurrent.futures module, multiprocessing, or even GPU acceleration using CUDA.\n",
    "\n",
    "**Example Code for Sequential Matrix Addition** : Before diving into parallelism, let’s look at how matrix  addition is implemented sequentially:"
   ],
   "id": "e1058297501be0e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T07:22:01.431199Z",
     "start_time": "2025-09-27T07:22:01.349564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from pandas.tests.plotting.test_backend import restore_backend\n",
    "\n",
    "# Define two matrix\n",
    "A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "B = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "C = A + B\n",
    "print(C)"
   ],
   "id": "bca05fae20839b77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  4  6]\n",
      " [ 8 10 12]\n",
      " [14 16 18]]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Parallel Matrix Addition Using Threads\n",
    "For parallelizing the matrix addition, we can divide the matrix\n",
    "into rows or blocks and assign each portion to a separate thread for computation. Here’s how you can\n",
    "do it using the `ThreadPoolExecutor` from the `concurrent.futures` module:"
   ],
   "id": "2357d2f51fc7618d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T07:25:42.532294Z",
     "start_time": "2025-09-27T07:25:42.520039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Function to add two matrices row by row\n",
    "def add_rows(row_a, row_b):\n",
    "    return row_a + row_b\n",
    "\n",
    "# Matrix addition with threading\n",
    "def parallel_matrix_addition(A,B):\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        result = list(executor.map(add_rows, A, B))\n",
    "    return np.array(result)\n",
    "\n",
    "A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "B = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# Perform parallel matrix addition\n",
    "C = parallel_matrix_addition(A, B)\n",
    "print(C)"
   ],
   "id": "79baf957eb610bfd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  4  6]\n",
      " [ 8 10 12]\n",
      " [14 16 18]]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Matrix Addition using CUDA\n",
    "We can further optimize matrix addition by leveraging CUDA for GPU\n",
    "acceleration. CUDA enables the use of GPUs to perform parallel matrix operations on a massive scale,\n",
    "making it much faster for large matrices. Below is an example of using CUDA for matrix addition:"
   ],
   "id": "37d68fe2ce315e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T10:05:57.833907Z",
     "start_time": "2025-09-27T10:05:57.715892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "# Define the matrix size\n",
    "N =3\n",
    "\n",
    "# cuda Kernel for Matrix addition\n",
    "@cuda.jit\n",
    "def matrix_addition(A, B, C):\n",
    "    i, j = cuda.grid(2)\n",
    "    if i < C.shape[0] and j < C.shape[1]:\n",
    "        C[i, j] = A[i, j] + B[i, j]\n",
    "\n",
    "# Initialize matrices\n",
    "A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "B = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "C = np.zeros((N, N), dtype=np.float32)\n",
    "\n",
    "# Define Grid and Block Sizes\n",
    "threads_per_block = (16,16)\n",
    "blocks_per_grid_x = int(np.ceil(A.shape[0] / threads_per_block[0])) #1\n",
    "blocks_per_grid_y = int(np.ceil(A.shape[1] / threads_per_block[1])) #1\n",
    "blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y)    #(1,1)\n",
    "\n",
    "# Call the cuda kernel\n",
    "matrix_addition[ blocks_per_grid, threads_per_block ](A, B, C)\n",
    "\n",
    "print(C)"
   ],
   "id": "8801a4762f806c44",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.  4.  6.]\n",
      " [ 8. 10. 12.]\n",
      " [14. 16. 18.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/san/miniconda/envs/mac-linux/lib/python3.13/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/home/san/miniconda/envs/mac-linux/lib/python3.13/site-packages/numba/cuda/cudadrv/devicearray.py:887: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Thread Grid Visualization\n",
    "We have:\n",
    "* 1 block containing 16×16 threads (256 total)\n",
    "* 3×3 matrix (9 elements)\n",
    "```\n",
    "Thread Block (16×16 threads) - Only top-left 3×3 are used:\n",
    "Thread Coordinates (i,j) within the block:\n",
    "(0,0) (0,1) (0,2) (0,3) ... (0,15)\n",
    "(1,0) (1,1) (1,2) (1,3) ... (1,15)\n",
    "(2,0) (2,1) (2,2) (2,3) ... (2,15)\n",
    "(3,0) (3,1) (3,2) (3,3) ... (3,15)\n",
    " ...   ...   ...   ...  ...   ...\n",
    "(15,0)(15,1)(15,2)(15,3)... (15,15)\n",
    "```\n",
    "* Matrix Mapping : Each thread gets global coordinates via cuda.grid(2). Since we have only 1 block, the mapping is direct:\n",
    "    * i = thread_x (0 to 15)\n",
    "    * j = thread_y (0 to 15)\n",
    "\n",
    "### Which Threads Actually Work?\n",
    "* **Working threads (9 threads):**\n",
    "```\n",
    "Matrix C indices ←→ Thread coordinates\n",
    "C[0,0] ← Thread (0,0) → 1+1=2\n",
    "C[0,1] ← Thread (0,1) → 2+2=4\n",
    "C[0,2] ← Thread (0,2) → 3+3=6\n",
    "C[1,0] ← Thread (1,0) → 4+4=8\n",
    "C[1,1] ← Thread (1,1) → 5+5=10\n",
    "C[1,2] ← Thread (1,2) → 6+6=12\n",
    "C[2,0] ← Thread (2,0) → 7+7=14\n",
    "C[2,1] ← Thread (2,1) → 8+8=16\n",
    "C[2,2] ← Thread (2,2) → 9+9=18\n",
    "```\n",
    "* **Non-working threads (247 threads):**\n",
    "    * Threads with i ≥ 3 OR j ≥ 3 skip the computation\n",
    "    * Examples: (0,3), (3,0), (15,15) - all hit the boundary check and exit\n",
    "\n",
    "### Visual Map of Active Threads\n",
    "```\n",
    "Active Threads/Matrix Elements:\n",
    "🟩 🟩 🟩 ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫\n",
    "🟩 🟩 🟩 ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫\n",
    "🟩 🟩 🟩 ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫\n",
    " ▫  ▫  ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫\n",
    " ... 237 more inactive threads ...\n",
    "```\n",
    "where 🟩 = Active thread (computes one matrix element), ▫ = Inactive thread (skips computation due to boundary check)\n",
    "* **Execution Pattern**\n",
    "* GPU launches 256 threads in parallel\n",
    "    * 9 threads find they have valid matrix indices (0-2, 0-2) and perform addition\n",
    "    * 247 threads immediately exit because their coordinates are outside the matrix bounds\n",
    "    * All threads complete simultaneously (GPU processes them in warps of 32)\n",
    "\n",
    "If we had a 16×16 matrix instead: All 256 threads would be utilized , Perfect mapping: one thread per matrix element , No wasted computation."
   ],
   "id": "29ce8b3966680e4c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Performance Warnings\n",
    "These are performance warnings, not errors! Your code will still run correctly, but Numba is giving you helpful suggestions to optimize GPU performance.\n",
    "\n",
    "### Warning 1: Low Occupancy\n",
    "`Grid size 1 will likely result in GPU under-utilization due to low occupancy.`\n"
   ],
   "id": "766e7ea452b3e40d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9ec0aa193589e98"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
