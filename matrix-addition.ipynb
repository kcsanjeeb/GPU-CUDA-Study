{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Matrix Addition\n",
    "Matrix addition is an element-wise operation, which makes it highly parallelizable. Each element in\n",
    "the resulting matrix can be computed independently. Using Python, we can parallelize this operation\n",
    "using the concurrent.futures module, multiprocessing, or even GPU acceleration using CUDA.\n",
    "\n",
    "**Example Code for Sequential Matrix Addition** : Before diving into parallelism, let’s look at how matrix  addition is implemented sequentially:"
   ],
   "id": "e1058297501be0e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T02:24:32.364619Z",
     "start_time": "2025-09-28T02:24:32.176793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from pandas.tests.plotting.test_backend import restore_backend\n",
    "\n",
    "# Define two matrix\n",
    "A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "B = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "C = A + B\n",
    "print(C)"
   ],
   "id": "bca05fae20839b77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  4  6]\n",
      " [ 8 10 12]\n",
      " [14 16 18]]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Parallel Matrix Addition Using Threads\n",
    "For parallelizing the matrix addition, we can divide the matrix\n",
    "into rows or blocks and assign each portion to a separate thread for computation. Here’s how you can\n",
    "do it using the `ThreadPoolExecutor` from the `concurrent.futures` module:"
   ],
   "id": "2357d2f51fc7618d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T02:24:32.406307Z",
     "start_time": "2025-09-28T02:24:32.399822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Function to add two matrices row by row\n",
    "def add_rows(row_a, row_b):\n",
    "    return row_a + row_b\n",
    "\n",
    "# Matrix addition with threading\n",
    "def parallel_matrix_addition(A,B):\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        result = list(executor.map(add_rows, A, B))\n",
    "    return np.array(result)\n",
    "\n",
    "A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "B = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# Perform parallel matrix addition\n",
    "C = parallel_matrix_addition(A, B)\n",
    "print(C)"
   ],
   "id": "79baf957eb610bfd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  4  6]\n",
      " [ 8 10 12]\n",
      " [14 16 18]]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Matrix Addition using CUDA\n",
    "We can further optimize matrix addition by leveraging CUDA for GPU\n",
    "acceleration. CUDA enables the use of GPUs to perform parallel matrix operations on a massive scale,\n",
    "making it much faster for large matrices. Below is an example of using CUDA for matrix addition:"
   ],
   "id": "37d68fe2ce315e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T02:24:32.520437Z",
     "start_time": "2025-09-28T02:24:32.470036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "# Define the matrix size\n",
    "N =3\n",
    "\n",
    "# cuda Kernel for Matrix addition\n",
    "@cuda.jit\n",
    "def matrix_addition(A, B, C):\n",
    "    i, j = cuda.grid(2)\n",
    "    if i < C.shape[0] and j < C.shape[1]:\n",
    "        C[i, j] = A[i, j] + B[i, j]\n",
    "\n",
    "# Initialize matrices\n",
    "A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "B = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "C = np.zeros((N, N), dtype=np.float32)\n",
    "\n",
    "# Define Grid and Block Sizes\n",
    "threads_per_block = (16,16)\n",
    "blocks_per_grid_x = int(np.ceil(A.shape[0] / threads_per_block[0])) #1\n",
    "blocks_per_grid_y = int(np.ceil(A.shape[1] / threads_per_block[1])) #1\n",
    "blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y)    #(1,1)\n",
    "\n",
    "# Call the cuda kernel\n",
    "matrix_addition[ blocks_per_grid, threads_per_block ](A, B, C)\n",
    "\n",
    "print(C)"
   ],
   "id": "8801a4762f806c44",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.  4.  6.]\n",
      " [ 8. 10. 12.]\n",
      " [14. 16. 18.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/san/miniconda/envs/mac-linux/lib/python3.13/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/home/san/miniconda/envs/mac-linux/lib/python3.13/site-packages/numba/cuda/cudadrv/devicearray.py:887: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Thread Grid Visualization\n",
    "We have:\n",
    "* 1 block containing 16×16 threads (256 total)\n",
    "* 3×3 matrix (9 elements)\n",
    "```\n",
    "Thread Block (16×16 threads) - Only top-left 3×3 are used:\n",
    "Thread Coordinates (i,j) within the block:\n",
    "(0,0) (0,1) (0,2) (0,3) ... (0,15)\n",
    "(1,0) (1,1) (1,2) (1,3) ... (1,15)\n",
    "(2,0) (2,1) (2,2) (2,3) ... (2,15)\n",
    "(3,0) (3,1) (3,2) (3,3) ... (3,15)\n",
    " ...   ...   ...   ...  ...   ...\n",
    "(15,0)(15,1)(15,2)(15,3)... (15,15)\n",
    "```\n",
    "* Matrix Mapping : Each thread gets global coordinates via cuda.grid(2). Since we have only 1 block, the mapping is direct:\n",
    "    * i = thread_x (0 to 15)\n",
    "    * j = thread_y (0 to 15)\n",
    "\n",
    "### Which Threads Actually Work?\n",
    "* **Working threads (9 threads):**\n",
    "```\n",
    "Matrix C indices ←→ Thread coordinates\n",
    "C[0,0] ← Thread (0,0) → 1+1=2\n",
    "C[0,1] ← Thread (0,1) → 2+2=4\n",
    "C[0,2] ← Thread (0,2) → 3+3=6\n",
    "C[1,0] ← Thread (1,0) → 4+4=8\n",
    "C[1,1] ← Thread (1,1) → 5+5=10\n",
    "C[1,2] ← Thread (1,2) → 6+6=12\n",
    "C[2,0] ← Thread (2,0) → 7+7=14\n",
    "C[2,1] ← Thread (2,1) → 8+8=16\n",
    "C[2,2] ← Thread (2,2) → 9+9=18\n",
    "```\n",
    "* **Non-working threads (247 threads):**\n",
    "    * Threads with i ≥ 3 OR j ≥ 3 skip the computation\n",
    "    * Examples: (0,3), (3,0), (15,15) - all hit the boundary check and exit\n",
    "\n",
    "### Visual Map of Active Threads\n",
    "```\n",
    "Active Threads/Matrix Elements:\n",
    "🟩 🟩 🟩 ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫\n",
    "🟩 🟩 🟩 ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫\n",
    "🟩 🟩 🟩 ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫\n",
    " ▫  ▫  ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫ ▫\n",
    " ... 237 more inactive threads ...\n",
    "```\n",
    "where 🟩 = Active thread (computes one matrix element), ▫ = Inactive thread (skips computation due to boundary check)\n",
    "* **Execution Pattern**\n",
    "* GPU launches 256 threads in parallel\n",
    "    * 9 threads find they have valid matrix indices (0-2, 0-2) and perform addition\n",
    "    * 247 threads immediately exit because their coordinates are outside the matrix bounds\n",
    "    * All threads complete simultaneously (GPU processes them in warps of 32)\n",
    "\n",
    "If we had a 16×16 matrix instead: All 256 threads would be utilized , Perfect mapping: one thread per matrix element , No wasted computation."
   ],
   "id": "29ce8b3966680e4c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Performance Warnings\n",
    "These are performance warnings, not errors! Your code will still run correctly, but Numba is giving you helpful suggestions to optimize GPU performance.\n",
    "\n",
    "### Warning 1: Low Occupancy\n",
    "`Grid size 1 will likely result in GPU under-utilization due to low occupancy.`\n",
    "* What it means: You're only using 1 block, but GPUs work best with multiple blocks to hide memory latency.\n",
    "* Why it happens: For a 3×3 matrix, you get:\n",
    "    * `ceil(3/16)` = 1 block in each dimension\n",
    "    * Total: 1×1 = 1 block\n",
    "* Solution: Use a smaller thread block size that better matches your small matrix:\n",
    "\n",
    "### Warning 2: Host Array Copy Overhead\n",
    "`Host array used in CUDA kernel will incur copy overhead to/from device.`\n",
    "* What it means: Numba is automatically copying your CPU arrays to GPU memory, which takes time.\n",
    "* Why it happens: You're using regular NumPy arrays instead of explicitly allocated GPU arrays.\n",
    "* Solution: Pre-allocate arrays on the GPU:"
   ],
   "id": "766e7ea452b3e40d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T02:24:32.640260Z",
     "start_time": "2025-09-28T02:24:32.557347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from numba import cuda\n",
    "import numpy as np\n",
    "\n",
    "# Allocate memory directly on GPU\n",
    "A_device = cuda.to_device(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=np.float32))\n",
    "B_device = cuda.to_device(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=np.float32))\n",
    "C_device = cuda.device_array((N, N), dtype=np.float32)  # Allocate empty array on GPU\n",
    "\n",
    "# Run kernel\n",
    "matrix_addition[blocks_per_grid, threads_per_block](A_device, B_device, C_device)\n",
    "\n",
    "# Copy result back to CPU\n",
    "C = C_device.copy_to_host()\n",
    "print(C)"
   ],
   "id": "9ec0aa193589e98",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.  4.  6.]\n",
      " [ 8. 10. 12.]\n",
      " [14. 16. 18.]]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Original Configuration: (16,16) threads per block\n",
    "```\n",
    "GPU EXECUTION - 1 BLOCK with 16×16 THREADS (256 threads total)\n",
    "\n",
    "BLOCK 0 (16×16 threads) - Only 9 threads work, 247 are idle:\n",
    "\n",
    "Thread Grid in Block 0:\n",
    "╔═══════════════════════════════════════════════════════════════════════════════╗\n",
    "║ (0,0) (0,1) (0,2) (0,3) (0,4) ... (0,15)                                     ║\n",
    "║ (1,0) (1,1) (1,2) (1,3) (1,4) ... (1,15)                                     ║\n",
    "║ (2,0) (2,1) (2,2) (2,3) (2,4) ... (2,15)                                     ║\n",
    "║ (3,0) (3,1) (3,2) (3,3) (3,4) ... (3,15)                                     ║\n",
    "║ ...   ...   ...   ...   ...   ... ...                                         ║\n",
    "║ (15,0)(15,1)(15,2)(15,3)(15,4)...(15,15)                                     ║\n",
    "╚═══════════════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "Matrix Mapping (3×3):\n",
    "╔═════════╗     Only these threads do work:\n",
    "║ 🟢 🟢 🟢 ║     🟢 = Thread computes matrix element\n",
    "║ 🟢 🟢 🟢 ║     ⚪ = Thread skips (out of bounds)\n",
    "║ 🟢 🟢 🟢 ║\n",
    "╚═════════╝     Rest of block: 247 ⚪⚪⚪ threads do nothing\n",
    "\n",
    "WARP EXECUTION (32 threads/warp):\n",
    "Warp 0: Threads (0,0)-(0,15), (1,0)-(1,15) → Only 6 threads work, 26 idle\n",
    "Warp 1: Threads (2,0)-(2,15), (3,0)-(3,3)  → Only 3 threads work, 29 idle\n",
    "Warps 2-7: All threads idle\n",
    "```\n",
    "#### Optimized Configuration: (4,4) threads per block\n",
    "```\n",
    "GPU EXECUTION - 1 BLOCK with 4×4 THREADS (16 threads total)\n",
    "\n",
    "BLOCK 0 (4×4 threads) - 9 threads work, 7 are idle:\n",
    "\n",
    "Thread Grid in Block 0:\n",
    "╔═════════════════════╗\n",
    "║ (0,0) (0,1) (0,2) (0,3) ║\n",
    "║ (1,0) (1,1) (1,2) (1,3) ║\n",
    "║ (2,0) (2,1) (2,2) (2,3) ║\n",
    "║ (3,0) (3,1) (3,2) (3,3) ║\n",
    "╚═════════════════════╝\n",
    "\n",
    "Matrix Mapping (3×3):\n",
    "╔═════════╗     Thread activity:\n",
    "║ 🟢 🟢 🟢 ⚪ ║     🟢 = Thread computes matrix element (i<3, j<3)\n",
    "║ 🟢 🟢 🟢 ⚪ ║     ⚪ = Thread skips (j≥3 or i≥3)\n",
    "║ 🟢 🟢 🟢 ⚪ ║\n",
    "║ ⚪ ⚪ ⚪ ⚪ ║\n",
    "╚═════════╝     Only 7 idle threads instead of 247!\n",
    "\n",
    "WARP EXECUTION (32 threads/warp):\n",
    "Warp 0: Threads (0,0)-(3,3) → 9 threads work, 7 idle (much better utilization!)\n",
    "```\n",
    "#### Execution Flow Comparison\n",
    "```\n",
    "ORIGINAL (16,16) threads/block:\n",
    "┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐\n",
    "│   BLOCK 0       │    │   Matrix 3×3    │    │   GPU Warps     │\n",
    "│ 256 threads     │    │   9 elements    │    │ 8 warps needed  │\n",
    "│                 │ →  │                 │ →  │                 │\n",
    "│ Only 9 work     │    │ Massive waste   │    │ Poor occupancy  │\n",
    "│ 247 idle        │    │ of resources    │    │ (12.5% useful)  │\n",
    "└─────────────────┘    └─────────────────┘    └─────────────────┘\n",
    "\n",
    "OPTIMIZED (4,4) threads/block:\n",
    "┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐\n",
    "│   BLOCK 0       │    │   Matrix 3×3    │    │   GPU Warps     │\n",
    "│ 16 threads      │    │   9 elements    │    │ 1 warp needed   │\n",
    "│                 │ →  │                 │ →  │                 │\n",
    "│ 9 work, 7 idle  │    │ Good fit        │    │ Better occupancy│\n",
    "│                 │    │                 │    │ (56% useful)    │\n",
    "└─────────────────┘    └─────────────────┘    └─────────────────┘\n",
    "```\n",
    "#### Key Differences\n",
    "| Aspect | Original (16,16) | \tOptimized (4,4) |\n",
    "| --- | --- | --- |\n",
    "| Threads per block |\t256 |\t16 |\n",
    "| Working threads |\t9 |\t9 |\n",
    "| Idle threads |\t247 |\t7 |\n",
    "| Thread utilization |\t3.5% |\t56% |\n",
    "| Warps used |\t8 warps |\t1 warp |\n",
    "| Warp utilization |\t12.5% |\t56% |\n",
    "| Memory usage |\tHigher |\tLower |\n",
    "| GPU occupancy |\tLow\t | Medium |\n",
    "\n",
    "### Why Multiple Blocks Matter\n",
    "* **Better occupancy:** GPU can schedule blocks across different Streaming Multiprocessors (SMs)\n",
    "* **Latency hiding:** When one block waits for memory, other blocks can execute\n",
    "* **Resource utilization:** More blocks = better use of GPU's parallel architecture\n",
    "\n",
    "### The Trade-off\n",
    "* More blocks = better GPU utilization, but more \"wasted\" threads at boundaries\n",
    "* Fewer blocks = less thread waste, but poor GPU occupancy"
   ],
   "id": "4ae5803f6c5ba208"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## what Streaming Multiprocessors (SMs) are and how block scheduling works.\n",
    "A GPU is composed of multiple Streaming Multiprocessors (SMs):\n",
    "#### GPU CHIP OVERVIEW:\n",
    "```\n",
    "╔═══════════════════════════════════════════════════╗\n",
    "║                 GPU Chip                          ║\n",
    "║  ┌─────────────┐ ┌─────────────┐ ┌─────────────┐  ║\n",
    "║  │    SM 0     │ │    SM 1     │ │    SM 2     │  ║\n",
    "║  │             │ │             │ │             │  ║\n",
    "║  │ - Cores     │ │ - Cores     │ │ - Cores     │  ║\n",
    "║  │ - Registers │ │ - Registers │ │ - Registers │  ║\n",
    "║  │ - Cache     │ │ - Cache     │ │ - Cache     │  ║\n",
    "║  └─────────────┘ └─────────────┘ └─────────────┘  ║\n",
    "║  ┌─────────────┐ ┌─────────────┐ ┌─────────────┐  ║\n",
    "║  │    SM 3     │ │    SM 4     │ │    SM 5     │  ║\n",
    "║  │             │ │             │ │             │  ║\n",
    "║  └─────────────┘ └─────────────┘ └─────────────┘  ║\n",
    "║                 ... etc ...                       ║\n",
    "╚═══════════════════════════════════════════════════╝\n",
    "```\n",
    "Each SM has:\n",
    "* Multiple CUDA cores (e.g., 64-128 cores per SM)\n",
    "* Limited registers and shared memory\n",
    "* Scheduler that manages warps (groups of 32 threads)\n",
    "\n",
    "### Single Block vs Multiple Blocks Execution\n",
    "#### Case 1: 1 Block (Your Original Code)\n",
    "```\n",
    "GPU with 6 SMs:\n",
    "SM0: [BLOCK 0 - 256 threads] ← Only this SM is working!\n",
    "SM1: [IDLE]\n",
    "SM2: [IDLE]\n",
    "SM3: [IDLE]\n",
    "SM4: [IDLE]\n",
    "SM5: [IDLE]\n",
    "\n",
    "Result: 1 SM at 100% load, 5 SMs completely idle → 16.7% GPU utilization\n",
    "```\n",
    "#### Case 2: 4 Blocks (Optimized Code)\n",
    "```\n",
    "GPU with 6 SMs:\n",
    "SM0: [BLOCK (0,0) - 4 threads] ← Multiple SMs working!\n",
    "SM1: [BLOCK (0,1) - 4 threads]\n",
    "SM2: [BLOCK (1,0) - 4 threads]\n",
    "SM3: [BLOCK (1,1) - 4 threads]\n",
    "SM4: [IDLE] ← Some SMs still idle, but much better\n",
    "SM5: [IDLE]\n",
    "\n",
    "Result: 4 SMs working, 2 idle → 66.7% GPU utilization\n",
    "```\n",
    "#### What Happens Inside an SM\n",
    "Each SM can handle multiple blocks simultaneously through warp scheduling:\n",
    "* Inside SM0 with 1 block (256 threads):\n",
    "    * Warps: [Warp0][Warp1][Warp2]...[Warp7] ← 8 warps total\n",
    "    * Scheduler: Tries to keep all warps busy, but they all depend on same resources\n",
    "* Inside SM0 with multiple blocks:\n",
    "    * Warps: [Block0-Warp0][Block1-Warp0][Block0-Warp1]... ← Mixed warps from different blocks\n",
    "    * Scheduler: More flexibility to hide memory latency"
   ],
   "id": "293d55cecc984fcb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9e20e315c4041adc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T02:24:32.669560Z",
     "start_time": "2025-09-28T02:24:32.667655Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ecbad3fd489ae4fe",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
