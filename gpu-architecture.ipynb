{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# The GPU Hierarchical Structure\n",
    "\n",
    "#### A GPU consists of several key components:\n",
    "• **Cores:** The fundamental processing units capable of executing individual instructions.\n",
    "• **Streaming Multiprocessors (SMs):** Groups of cores that work together to execute tasks in parallel.\n",
    "• **Memory Hierarchy:** Different levels of memory, such as global memory, shared memory, and registers, optimize data access during computation."
   ],
   "id": "d0a892316c5a426e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here is a simplified overview of the GPU processing pipeline:\n",
    "1. **Data Input:** Data is passed to the GPU, typically from the system’s main memory (RAM) to the\n",
    "GPU’s global memory.\n",
    "2. **Kernel Execution:** The GPU processes data through kernels, which are small programs that run\n",
    "in parallel across thousands of cores.\n",
    "3. **Thread Management:** Threads are assigned to the cores, where each thread processes a small\n",
    "portion of the overall data.\n",
    "4. **Memory Access:** Data is read from and written to the memory hierarchy, including shared memory\n",
    "and registers for optimal performance.\n",
    "5. **Data Output:** After processing, the data is returned to the system memory or used in further GPU computations.\n",
    "\n",
    "The GPU processing pipeline differs from the CPU pipeline mainly in how it handles parallelism.\n",
    "While a CPU focuses on executing a few instructions quickly with a deep pipeline and high clock\n",
    "speeds, the GPU prioritizes executing many instructions simultaneously with many cores working in\n",
    "parallel."
   ],
   "id": "264dd8ce6437898d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Streaming Multiprocessors (SMs)\n",
    "At the heart of modern GPUs are Streaming Multiprocessors (SMs). Each SM contains a group of\n",
    "cores that can execute threads in parallel, allowing the GPU to handle massive parallelism efficiently.\n",
    "An SM consists of:\n",
    "* Cores: Individual processing units that execute threads.\n",
    "* Warp Scheduler: Determines how threads are grouped into warps, which are sets of 32 threads\n",
    "that execute the same instruction simultaneously.\n",
    "* Registers: Fast, low-latency memory used by individual threads for storing variables.\n",
    "* Shared Memory: A small, user-managed cache that allows threads within a block to share data\n",
    "and communicate efficiently.\n",
    "\n",
    "Each SM is designed to execute multiple threads simultaneously, maximizing parallelism and ensuring\n",
    "that the GPU can handle tasks like matrix multiplications, convolution operations, and other\n",
    "compute-heavy tasks in parallel."
   ],
   "id": "760fccc8989d1862"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Understanding Grid and Blocks in CUDA\n",
    "In CUDA (Compute Unified Device Architecture), computation is organized into a hierarchy of grids\n",
    "and blocks. This hierarchical structure allows the GPU to break down complex problems into smaller,\n",
    "manageable pieces, enabling large-scale parallelism.\n",
    "\n",
    "#### Defining the Grid\n",
    "The grid is the highest-level abstraction in CUDA’s execution model. It represents the overall problem\n",
    "space that is being solved on the GPU. A grid is composed of multiple blocks, which are further\n",
    "subdivided into threads.\n",
    "For example, if you are performing an image processing task on a 1024x1024 pixel image, the entire\n",
    "image could be represented as a grid, where each pixel is processed by a different thread.\n",
    "\n",
    "#### Blocks: The Subdivision of Grids\n",
    "Each grid in CUDA is subdivided into blocks. A block is a collection of threads that can execute independently\n",
    "on the GPU. Each block is assigned to an SM, which executes the threads of that block in\n",
    "parallel.\n",
    "A key feature of blocks is that they can communicate with each other using shared memory, which\n",
    "is faster than accessing global memory. This allows blocks to collaborate on a subset of the overall\n",
    "computation."
   ],
   "id": "3212a4b3ac5334e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T08:22:29.879Z",
     "start_time": "2025-09-26T08:22:29.765455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "@cuda.jit                                                                           # Compiles this function to run on GPU\n",
    "def matrix_addition(a, b, result):\n",
    "    # Get the index of the current thread in the grid\n",
    "    i, j = cuda.grid(2)                                                             # Each thread gets unique (i,j) coordinates\n",
    "\n",
    "    # Perform addition if the index is within the bounds\n",
    "    if i < result.shape[0] and j < result.shape[1]:\n",
    "        result[i, j] = a[i, j] + b[i, j]\n",
    "\n",
    "# Initialize data\n",
    "N = 1024\n",
    "a = np.random.rand(N, N)        # initialize value a with shape a = (1024, 1024)\n",
    "b = np.random.rand(N, N)        # initialize value b with shape b = (1024, 1024)\n",
    "result = np.zeros((N, N))       # initialize result with 0 values with shape result = (1024, 1024)\n",
    "\n",
    "# Define the grid and block dimensions / Calculate how many blocks needed to cover entire matrix\n",
    "threads_per_block = (16, 16)    # A block is 16x16 threads # 256 threads per block\n",
    "blocks_per_grid_x = (a.shape[0] + threads_per_block[0] - 1) // threads_per_block[0]\n",
    "blocks_per_grid_y = (a.shape[1] + threads_per_block[1] - 1) // threads_per_block[1]\n",
    "blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y)                            # Grid of thread blocks\n",
    "\n",
    "print(f\"Grid: {blocks_per_grid} blocks × {threads_per_block} threads\")\n",
    "print(f\"Total threads: {blocks_per_grid_x * blocks_per_grid_y * 16 * 16}\")\n",
    "\n",
    "# Launch the kernel\n",
    "matrix_addition[blocks_per_grid, threads_per_block](a, b, result)\n",
    "\n",
    "print(result)"
   ],
   "id": "d17d11eda36a40e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result = (1024, 1024)\n",
      "Grid: (64, 64) blocks × (16, 16) threads\n",
      "Total threads: 1048576\n",
      "[[1.36980796 1.14064583 1.85368593 ... 1.49270892 0.56310749 0.7460153 ]\n",
      " [1.71873084 0.59909694 0.75783558 ... 1.73687974 1.50361012 1.73527891]\n",
      " [0.57410286 0.96230128 1.13313017 ... 0.31919318 1.1129677  1.48177356]\n",
      " ...\n",
      " [0.36844851 0.58703067 1.67605846 ... 0.9899919  0.47099114 0.98850427]\n",
      " [1.14627379 1.26879155 1.74540994 ... 0.90718099 1.14108572 1.09423898]\n",
      " [0.48536405 1.39498137 1.12177176 ... 1.17229426 0.53833065 1.07447603]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/san/miniconda/envs/mac-linux/lib/python3.13/site-packages/numba/cuda/cudadrv/devicearray.py:887: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* The formula (n + block_size - 1) // block_size is a trick for ceiling division using integer arithmetic.\n",
    "    * **Regular division (floor):**\n",
    "        * 1024 / 16 = 64.0 → 64 blocks\n",
    "    * But 64 blocks × 16 threads = 1024 threads → Perfect fit!\n",
    "        * So for 1024, both floor and ceiling give same result"
   ],
   "id": "e81ac7f51904dd0e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 1. The @cuda.jit Decorator\n",
    "* What it does:\n",
    "    * Compiles the Python function to GPU machine code (PTX assembly)\n",
    "    * Creates a GPU kernel that can be launched with thousands of threads\n",
    "    * Handles memory transfers automatically between CPU and GPU\n",
    "* Without `@cuda.jit`: This is a normal Python function that runs sequentially on CPU\n",
    "* With `@cuda.jit`: This becomes a GPU kernel that runs in parallel on thousands of GPU cores\n",
    "\n",
    "#### 2. cuda.grid(2) - Thread Coordinate System\n",
    "* What's happening:\n",
    "    * GPU launches 1,048,576 threads (for 1024×1024 matrix)\n",
    "    * Each thread gets unique (i, j) coordinates based on its position in the grid\n",
    "    * Thread (0,0) processes element [0,0]\n",
    "    * Thread (0,1) processes element [0,1]\n",
    "    * Thread (15,15) processes element [15,15]\n",
    "    * Thread (16,0) processes element [16,0] (next block)\n",
    "\n",
    "#### 3. Bounds Checking - Critical Safety\n",
    "* Why this is necessary: We launch more threads than needed to handle uneven divisions.\n",
    "* Example: For a 1030×1030 matrix with 16×16 blocks:\n",
    "    * We need 65×65 blocks = 1,082,256 threads\n",
    "    * But only 1030×1030 = 1,060,900 elements exist\n",
    "    * Extra 21,356 threads hit this condition and do nothing\n",
    "* Without bounds check: Threads would try to access invalid memory → CRASH!\n",
    "\n",
    "#### 4. The Parallel Magic: result[i, j] = a[i, j] + b[i, j]\n",
    "* ALL 1 MILLION additions happen SIMULTANEOUSLY:\n",
    "* Thread(0,0):   result[0][0] = a[0][0] + b[0][0]\n",
    "* Thread(0,1):   result[0][1] = a[0][1] + b[0][1]\n",
    "* Thread(0,2):   result[0][2] = a[0][2] + b[0][2]\n",
    "* ...\n",
    "* Thread(1023,1023): result[1023][1023] = a[1023][1023] + b[1023][1023]\n",
    "* ↑ ALL HAPPEN AT THE SAME TIME!\n",
    "\n",
    "#### 5. Kernel Launch: [blocks_per_grid, threads_per_block]\n",
    "* This syntax is SPECIAL CUDA syntax (not normal Python):\n",
    "    * Normal Python function call: `function_name(arguments)`\n",
    "    * CUDA Kernel launch: `kernel_name[GRID_CONFIG](arguments)`\n",
    "        * What happens during launch:\n",
    "            * Copy input arrays (a, b) from CPU → GPU\n",
    "            * Allocate result array on GPU\n",
    "            * Launch 64×64×16×16 = 1,048,576 threads\n",
    "            * All threads execute the SAME code on DIFFERENT data\n",
    "            * Copy result array from GPU → CPU"
   ],
   "id": "49ab159f991c26c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5d801c855b883b4f"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
