{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Advanced CUDA Features and Optimization Techniques",
   "id": "d1aeb3426888decf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Streams and Concurrency\n",
    "CUDA streams provide a mechanism to overlap computation and data transfer, allowing us to optimize the GPU’s utilization by performing multiple tasks in parallel. The basic idea is that instead of serializing operations on the GPU (like launching one kernel and waiting for its completion), we can split tasks across multiple streams and execute them concurrently.\n",
    "\n",
    "## Overlapping Computation and Data Transfer\n",
    "By default, CUDA operates in a synchronous manner: memory transfers (from host to device or device to host) and kernel executions are serialized, meaning one must finish before the other begins. However, we can **overlap memory transfers with kernel execution using streams**, which allows for more efficient use of the GPU.\n",
    "\n",
    "To demonstrate overlapping, let’s look at the following example."
   ],
   "id": "5ff2d359859652c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "# Define the Kernel\n",
    "mod = SourceModule(\"\"\"\n",
    "__global__ void kernel(float *a, float *b){\n",
    "    int idx = threadIdx.x + blockDim.x * blockDim.x;\n",
    "    b[idx] = a[idx] * 2;\n",
    "}\n",
    "\"\"\")\n",
    "kernel = mod.get_function(\"kernel\")\n",
    "\n",
    "# Initialize host data\n",
    "N = 1024\n",
    "h_a = np.random.randn(N).astype(np.float32)\n",
    "h_b = np.empty_like(h_a)\n",
    "\n",
    "# Allocate Device memory\n",
    "d_a = cuda.mem_alloc(h_a.nbytes)\n",
    "d_b = cuda.mem_alloc(h_b.nbytes)\n",
    "\n",
    "# Create Streams\n",
    "stream1 = cuda.Stream()\n",
    "stream2 = cuda.Stream()\n",
    "\n",
    "# Transfer data asynchronously in stream1\n",
    "cuda.memcpy_htod_async(d_a, h_a, stream1)\n",
    "\n",
    "# Launch Kernel in Stream2\n",
    "kernel(d_a, d_b, block=(256, 1, 1), grid=(N // 256, 1), stream=stream2)\n",
    "\n",
    "# Transfer result back asynchronously in stream1\n",
    "cuda.mem_cpy_htod_async(d_b, h_b, stream1)\n",
    "\n",
    "# Synchronize the streams\n",
    "stream1.synchronize()\n",
    "stream2.synchronize()\n",
    "\n",
    "# Output Result\n",
    "print(h_b[:10])"
   ],
   "id": "49c8e4d4fa5978e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T09:57:03.657291Z",
     "start_time": "2025-09-28T09:57:03.645727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "N = 1024\n",
    "h_a = np.random.randn(N).astype(np.float32)\n",
    "\n",
    "# Create streams\n",
    "stream1 = torch.cuda.Stream()\n",
    "stream2 = torch.cuda.Stream()\n",
    "\n",
    "# Use streams\n",
    "with torch.cuda.stream(stream1):\n",
    "    d_a = torch.from_numpy(h_a).to(device)\n",
    "\n",
    "with torch.cuda.stream(stream2):\n",
    "    d_b = d_a * 2  # Automatic GPU operation\n",
    "\n",
    "# Synchronize and copy back\n",
    "torch.cuda.synchronize()\n",
    "h_b = d_b.cpu().numpy()\n",
    "\n",
    "print(h_b[:10])"
   ],
   "id": "6a14b9f9a4bca88c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.24536662  2.7647512  -0.29380977 -2.7479107   1.5783794   3.2773077\n",
      "  0.75366443  3.4507585  -2.7262783   1.6730118 ]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**In this example: -** We created two streams: stream1 and stream2. - Data transfer (host to device\n",
    "and device to host) occurs in stream1, and kernel execution happens in stream2. - By running these op-\n",
    "erations concurrently in different streams, we achieve better utilization of both the memory bandwidth\n",
    "and computational power of the GPU"
   ],
   "id": "57499ec34b5930a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Managing Multiple Streams\n",
    "Managing multiple streams becomes essential when optimizing more complex applications. CUDA\n",
    "streams are independent, and operations issued to different streams can be executed concurrently.\n",
    "However, there may be cases where we want to ensure the correct order of execution between streams.\n",
    "To manage dependencies between streams, we can use cudaStreamWaitEventto synchronize streams\n",
    "based on certain events. This ensures that kernels in one stream only start after a specific event in\n",
    "another stream has occurred.\n",
    "\n",
    "Here’s a simple example:"
   ],
   "id": "8752a504f87d3831"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Creating an event\n",
    "event = cuda.Event()\n",
    "\n",
    "cuda.memcpy_htod_async(d_a, h_a, stream1)\n",
    "event.record(stream1)\n",
    "\n",
    "# Make stream2 wait for stream1 to finish\n",
    "stream2.wait_event(event)\n",
    "\n",
    "# Now we can safely execute the kernel in stream2\n",
    "kernel(d_a, d_b, block=(256, 1, 1), grid=(N // 256, 1), stream=stream2)"
   ],
   "id": "26feb62ba56be98e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this code, we used an event to ensure that stream2 only starts its kernel after the data transfer\n",
    "in stream1 is complete."
   ],
   "id": "5ebfafc61d4095c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Dynamic Parallelism\n",
    "Dynamic parallelism in CUDA allows a kernel to launch other kernels directly from the device. This is\n",
    "useful for algorithms where the problem size is not known in advance or is irregular, such as adaptive\n",
    "mesh refinement, graph traversal, or recursive algorithms."
   ],
   "id": "b76b45ac981018f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Using Profilers to Identify Bottlenecks\n",
    "Profilers provide insights into execution time, memory usage, and other performance metrics. By\n",
    "analyzing the output of these tools, we can pinpoint areas where our application is underperforming.\n",
    "A common scenario is identifying memory transfer bottlenecks. For example, using nvprof:\n",
    "``` nvprof python cuda_program.py ```\n",
    "The profiler will show detailed information about the kernel’s execution time, memory transfer time,\n",
    "and any potential bottlenecks in the code."
   ],
   "id": "ce32bf74af89d673"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "! nvprof python  ./cpp-cuda/vector-addition",
   "id": "1952b1a6d37cb33c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a3c69c382f398181"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
